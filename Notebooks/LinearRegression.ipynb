{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Say we have a dataset and we decide to plot it as so:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/cricketpoints.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this particular case, the x variable is the amount of cricket chirps per minute, and the y variable is the temperature in celsius. \n",
    "\n",
    "We can draw a line \"through\" the data points, as such:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/CricketLine.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows us that the relationship between chirps per minute and the temperature in celisus is linear. Using mathematics, we know that the equation for a line is\n",
    "> y = mx + b\n",
    "- y is the temperature in celsius\n",
    "- x represents the cricket chirps per minute\n",
    "- b represents the y intercept\n",
    "- m is the slope of the line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In ML, the equation is a bit different:\n",
    "> y' = b + w1x1\n",
    "- y' is the label that we are predicting\n",
    "- b is the bias (y intercept)\n",
    "- w1 is the weight of feature one. Weight is the same concept of slope \"m\"\n",
    "- x1 is the feature (known input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this particular case, our model has only one feature, but in the real world, models can have up to millions of features, denoted\n",
    "> y' = b + w<sub>1</sub>x<sub>1</sub> + w<sub>2</sub>x<sub>2</sub> + w<sub>3</sub>x<sub>3</sub>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training a model simply means learning (determining) good values for all the weights and the bias from labeled examples**. In supervised learning, an ML algorithm builds a model by examining many examples and attempting to find a model that minimizes **loss**. This process is called empirical risk minimization. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss is the penalty for a bad prediction. In other words, loss tells us how bad the model's prediction was for a single example. A perfect prediction means a loss of zero, otherwise the loss is greater. \n",
    "\n",
    "**The overall goal of training a model is to find a set of weights and biases that have low loss, on average, across all examples.**\n",
    "\n",
    "For example, see the charts below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Loss](img/LossSidebySide.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In the charts above, the one on the left has high loss, while the one on the right has low loss**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a way to create a mathematical function that allows us to aggregate the individual losses in a meaningful manner. In other words, there is a way for us to visualize loss such that we know how to counter it, and that is where our **loss function** comes into play."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Squared Loss: A Popular Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The linear regression models examined in these notes use a loss function called **squared loss** (L2 loss). The squared loss for any single example is as follows:\n",
    "\n",
    "= The square of the difference between the label and the prediction  \n",
    "= (observation - prediction(x))<sup>2</sup>  \n",
    "= **(y - y')<sup>2</sup>**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Square Error (MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MSE is the average squared loss per example in a dataset. To calculate MSE, sum up all of the squared losses for individual examples and then divide by the number of examples:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![mse](img/mse.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In this formula, the sigma means 'sum'. Therefore, it could be said that MSE is equal to 1/n times the sum of (y-prediction(x))<sup>2</sup> for every pair (x, y) in the set D.**\n",
    "\n",
    "- (x, y) is an example in which \n",
    "    - x is the set of features that the model uses to make predictions\n",
    "    - y is the example's label\n",
    "- prediction(x) is a function of the weights and bias in combination with the set features of x\n",
    "- D is a data set of many labeled examples, which are (x, y) pairs\n",
    "- N is the number of examples in D\n",
    "\n",
    "Although MSE is commonly used in ML, it is not the only nor the best loss function available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducing Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most important parts of training a ML algorithm is reducing loss. There are many different ways to reduce loss, but for this example, we will look at one approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterative Approach\n",
    "This can best be compared to a giant game of \"hot and cold\". In essence, the model starts with a random weight and bias, and then plugs that prediction into a loss function. With the loss it computes, it changes the values of the weights and bias in order to attempt to reduce loss. It does this over and over again (hence iterative) until the loss is 0 or stops declining or increasing at all. See this diagram: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](img/IterativeDiagram.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"compute loss\" part of the diagram does just that. It uses a loss function to calculate the loss for a given example of b and w<sub>1</sub>. After computing the loss, it feeds it to another function that computes new values for b and w<sub>1</sub>.  \n",
    "\n",
    "Once the loss reaches zero, stops changing, or changes extremely slowly, the model knows that it has **converged**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
